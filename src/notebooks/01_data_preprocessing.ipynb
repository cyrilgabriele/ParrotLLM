{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Data Preprocessing & EDA\n",
    "\n",
    "**Goal:** Explore OpenWebText, filter it, tokenize it, and produce clean train/val splits.\n",
    "\n",
    "Steps:\n",
    "1. Load dataset (small subset first, then full)\n",
    "2. EDA: document lengths, token distributions, language stats\n",
    "3. Filter: English only, remove test set overlap\n",
    "4. Tokenize with GPT-2 tokenizer\n",
    "5. Save as binary files for fast training\n",
    "\n",
    "Once this works â†’ extract into `src/data/preprocess.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/christof/ParrotLLM\n",
      "Data dir: /Users/christof/ParrotLLM/data\n",
      "Contents: [PosixPath('/Users/christof/ParrotLLM/data/lid.176.ftz'), PosixPath('/Users/christof/ParrotLLM/data/openwebtext-10k'), PosixPath('/Users/christof/ParrotLLM/data/wikitext-103-test')]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import GPT2TokenizerFast\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# All paths relative to project root\n",
    "ROOT = Path(\"../..\").resolve()\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "print(f\"Project root: {ROOT}\")\n",
    "print(f\"Data dir: {DATA_DIR}\")\n",
    "print(f\"Contents: {list(DATA_DIR.iterdir())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset\n",
    "\n",
    "Start with the 10k subset for fast iteration. Switch to full later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 10000\n",
      "Columns: ['text']\n",
      "\n",
      "Example document (first 500 chars):\n",
      "Port-au-Prince, Haiti (CNN) -- Earthquake victims, writhing in pain and grasping at life, watched doctors and nurses walk away from a field hospital Friday night after a Belgian medical team evacuated the area, saying it was concerned about security.\n",
      "\n",
      "The decision left CNN Chief Medical Correspondent Sanjay Gupta as the only doctor at the hospital to get the patients through the night.\n",
      "\n",
      "CNN initially reported, based on conversations with some of the doctors, that the United Nations ordered the B\n"
     ]
    }
   ],
   "source": [
    "# Load from local data/ folder (downloaded by scripts/download_data.py)\n",
    "ds_small = load_from_disk(str(DATA_DIR / \"openwebtext-10k\"))\n",
    "print(f\"Documents: {len(ds_small)}\")\n",
    "print(f\"Columns: {ds_small.column_names}\")\n",
    "print(f\"\\nExample document (first 500 chars):\")\n",
    "print(ds_small[0][\"text\"][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenizer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 50257\n",
      "Example: 'Hello world' -> [15496, 995]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"Example: 'Hello world' -> {tokenizer.encode('Hello world')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. EDA: Document & Token Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1217 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in 10k subset: 11,133,993\n",
      "Mean doc length: 1113 tokens\n",
      "Median doc length: 716 tokens\n",
      "Min: 148, Max: 45207\n",
      "Docs > 1024 tokens: 3270 (32.7%)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize a sample and look at length distribution\n",
    "doc_lengths = []\n",
    "for doc in ds_small:\n",
    "    tokens = tokenizer.encode(doc[\"text\"])\n",
    "    doc_lengths.append(len(tokens))\n",
    "\n",
    "doc_lengths = np.array(doc_lengths)\n",
    "print(f\"Total tokens in 10k subset: {doc_lengths.sum():,}\")\n",
    "print(f\"Mean doc length: {doc_lengths.mean():.0f} tokens\")\n",
    "print(f\"Median doc length: {np.median(doc_lengths):.0f} tokens\")\n",
    "print(f\"Min: {doc_lengths.min()}, Max: {doc_lengths.max()}\")\n",
    "print(f\"Docs > 1024 tokens: {(doc_lengths > 1024).sum()} ({(doc_lengths > 1024).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated total tokens in full OWT: 8,922,524,794 (~8.9B)\n"
     ]
    }
   ],
   "source": [
    "# Extrapolate to full dataset\n",
    "# Full OWT has ~8M docs. Our 10k sample gives us a rough estimate.\n",
    "full_docs = 8_013_769\n",
    "est_total_tokens = int(doc_lengths.mean() * full_docs)\n",
    "print(f\"Estimated total tokens in full OWT: {est_total_tokens:,} (~{est_total_tokens/1e9:.1f}B)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Language Detection\n",
    "\n",
    "Filter out non-English documents. Use fasttext's language detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fasttext'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfasttext\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Load from local data/ folder (downloaded by scripts/download_data.py)\u001b[39;00m\n\u001b[32m      4\u001b[39m model_path = \u001b[38;5;28mstr\u001b[39m(DATA_DIR / \u001b[33m\"\u001b[39m\u001b[33mlid.176.ftz\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'fasttext'"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "\n",
    "# Load from local data/ folder (downloaded by scripts/download_data.py)\n",
    "model_path = str(DATA_DIR / \"lid.176.ftz\")\n",
    "lang_model = fasttext.load_model(model_path)\n",
    "print(f\"Loaded language detection model from {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text, model):\n",
    "    \"\"\"Return detected language code and confidence.\"\"\"\n",
    "    # fasttext needs single line, no newlines\n",
    "    first_line = text.split(\"\\n\")[0].strip()\n",
    "    if not first_line:\n",
    "        return \"unknown\", 0.0\n",
    "    pred = model.predict(first_line)\n",
    "    lang = pred[0][0].replace(\"__label__\", \"\")\n",
    "    conf = pred[1][0]\n",
    "    return lang, conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lang_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m non_english = []\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(ds_small):\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     lang, conf = detect_language(doc[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m], \u001b[43mlang_model\u001b[49m)\n\u001b[32m      7\u001b[39m     lang_counts[lang] += \u001b[32m1\u001b[39m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m lang != \u001b[33m\"\u001b[39m\u001b[33men\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mNameError\u001b[39m: name 'lang_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Check language distribution in our sample\n",
    "lang_counts = Counter()\n",
    "non_english = []\n",
    "\n",
    "for i, doc in enumerate(ds_small):\n",
    "    lang, conf = detect_language(doc[\"text\"], lang_model)\n",
    "    lang_counts[lang] += 1\n",
    "    if lang != \"en\":\n",
    "        non_english.append((i, lang, conf, doc[\"text\"][:100]))\n",
    "\n",
    "print(\"Language distribution (top 10):\")\n",
    "for lang, count in lang_counts.most_common(10):\n",
    "    print(f\"  {lang}: {count} ({count/len(ds_small)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nNon-English documents: {len(non_english)} ({len(non_english)/len(ds_small)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample non-English documents:\n"
     ]
    }
   ],
   "source": [
    "# Look at some non-English examples\n",
    "print(\"Sample non-English documents:\")\n",
    "for idx, lang, conf, preview in non_english[:5]:\n",
    "    print(f\"  [{lang} conf={conf:.2f}] {preview}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Set Decontamination\n",
    "\n",
    "**CRITICAL:** Remove any overlap between training data and test sets.\n",
    "\n",
    "Test sets to exclude:\n",
    "- Wikitext-103 test split\n",
    "- NLP26 OpenWebText eval split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Wikitext-103 test set from local data/ folder\n",
    "wiki_test = load_from_disk(str(DATA_DIR / \"wikitext-103-test\"))\n",
    "print(f\"Wikitext-103 test documents: {len(wiki_test)}\")\n",
    "print(f\"Example: {wiki_test[0]['text'][:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a set of n-grams from test sets for overlap detection\n",
    "def extract_ngrams(text, n=13):\n",
    "    \"\"\"Extract character-level n-grams from text for contamination detection.\"\"\"\n",
    "    text = text.lower().strip()\n",
    "    return {text[i:i+n] for i in range(len(text) - n + 1)}\n",
    "\n",
    "# Build test set n-gram index\n",
    "test_ngrams = set()\n",
    "for doc in wiki_test:\n",
    "    if doc[\"text\"].strip():\n",
    "        test_ngrams.update(extract_ngrams(doc[\"text\"]))\n",
    "\n",
    "print(f\"Test set n-grams: {len(test_ngrams):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Also load and index the NLP26 OWT eval split\n",
    "# Download from: https://drive.switch.ch/index.php/s/6TLGQFEIkAPJ72K\n",
    "# Place in data/ directory\n",
    "\n",
    "# owt_eval = load_dataset(\"path/to/eval/split\")\n",
    "# for doc in owt_eval:\n",
    "#     test_ngrams.update(extract_ngrams(doc[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_contaminated(text, test_ngrams, n=13, threshold=0.8):\n",
    "    \"\"\"Check if a document has high overlap with test set n-grams.\"\"\"\n",
    "    doc_ngrams = extract_ngrams(text, n)\n",
    "    if not doc_ngrams:\n",
    "        return False\n",
    "    overlap = len(doc_ngrams & test_ngrams) / len(doc_ngrams)\n",
    "    return overlap > threshold\n",
    "\n",
    "# Test on our sample\n",
    "contaminated = sum(1 for doc in ds_small if is_contaminated(doc[\"text\"], test_ngrams))\n",
    "print(f\"Contaminated documents in 10k sample: {contaminated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Full Preprocessing Pipeline (preview)\n",
    "\n",
    "Combine all filters and tokenize. This is what will become `src/data/preprocess.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_document(doc, tokenizer, lang_model, test_ngrams):\n",
    "    \"\"\"Process one document. Returns token IDs or None if filtered out.\"\"\"\n",
    "    text = doc[\"text\"]\n",
    "\n",
    "    # Filter: skip empty\n",
    "    if not text.strip():\n",
    "        return None, \"empty\"\n",
    "\n",
    "    # Filter: English only\n",
    "    lang, conf = detect_language(text, lang_model)\n",
    "    if lang != \"en\" or conf < 0.5:\n",
    "        return None, \"non_english\"\n",
    "\n",
    "    # Filter: test set contamination\n",
    "    if is_contaminated(text, test_ngrams):\n",
    "        return None, \"contaminated\"\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = tokenizer.encode(text)\n",
    "\n",
    "    # Filter: too short\n",
    "    if len(tokens) < 64:\n",
    "        return None, \"too_short\"\n",
    "\n",
    "    return tokens, \"kept\"\n",
    "\n",
    "\n",
    "# Run on sample\n",
    "from collections import Counter\n",
    "stats = Counter()\n",
    "all_tokens = []\n",
    "\n",
    "for doc in ds_small:\n",
    "    tokens, status = preprocess_document(doc, tokenizer, lang_model, test_ngrams)\n",
    "    stats[status] += 1\n",
    "    if tokens is not None:\n",
    "        all_tokens.extend(tokens)\n",
    "\n",
    "print(\"Preprocessing stats:\")\n",
    "for status, count in stats.most_common():\n",
    "    print(f\"  {status}: {count} ({count/len(ds_small)*100:.1f}%)\")\n",
    "print(f\"\\nTotal tokens kept: {len(all_tokens):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save as Binary\n",
    "\n",
    "Save tokenized data as a memory-mapped numpy array for fast training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy uint16 (vocab size 50257 fits in uint16 max=65535)\n",
    "token_array = np.array(all_tokens, dtype=np.uint16)\n",
    "print(f\"Array shape: {token_array.shape}\")\n",
    "print(f\"Size on disk: {token_array.nbytes / 1e6:.1f} MB\")\n",
    "\n",
    "# Save (for the real pipeline, split into train.bin and val.bin)\n",
    "# out_dir = DATA_DIR / \"processed\"\n",
    "# out_dir.mkdir(exist_ok=True)\n",
    "# token_array.tofile(str(out_dir / \"train.bin\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Run this on the **full** OpenWebText dataset: `load_dataset(\"Skylion007/openwebtext\")`\n",
    "2. Add the NLP26 eval split to decontamination\n",
    "3. Create train/val split (99%/1%)\n",
    "4. Extract working code into `src/data/preprocess.py`\n",
    "5. Wire into `python main.py --stage preprocess --dataset-size full --lang en`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
