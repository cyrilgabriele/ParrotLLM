{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Data Preprocessing & EDA\n",
    "\n",
    "**Goal:** Explore OpenWebText, filter it, tokenize it, and produce clean train/val splits.\n",
    "\n",
    "Steps:\n",
    "1. Load dataset (small subset first, then full)\n",
    "2. EDA: document lengths, token distributions, language stats\n",
    "3. Filter: English only, remove test set overlap\n",
    "4. Tokenize with GPT-2 tokenizer\n",
    "5. Save as binary files for fast training\n",
    "\n",
    "Once this works â†’ extract into `src/data/preprocess.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/christof/ParrotLLM\n",
      "Data dir: /Users/christof/ParrotLLM/data\n",
      "Contents: [PosixPath('/Users/christof/ParrotLLM/data/lid.176.ftz'), PosixPath('/Users/christof/ParrotLLM/data/openwebtext-10k'), PosixPath('/Users/christof/ParrotLLM/data/wikitext-103-test')]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import GPT2TokenizerFast\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# All paths relative to project root\n",
    "ROOT = Path(\"../..\").resolve()\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "print(f\"Project root: {ROOT}\")\n",
    "print(f\"Data dir: {DATA_DIR}\")\n",
    "print(f\"Contents: {list(DATA_DIR.iterdir())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset\n",
    "\n",
    "Start with the 10k subset for fast iteration. Switch to full later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 10000\n",
      "Columns: ['text']\n",
      "\n",
      "Example document (first 500 chars):\n",
      "Port-au-Prince, Haiti (CNN) -- Earthquake victims, writhing in pain and grasping at life, watched doctors and nurses walk away from a field hospital Friday night after a Belgian medical team evacuated the area, saying it was concerned about security.\n",
      "\n",
      "The decision left CNN Chief Medical Correspondent Sanjay Gupta as the only doctor at the hospital to get the patients through the night.\n",
      "\n",
      "CNN initially reported, based on conversations with some of the doctors, that the United Nations ordered the B\n"
     ]
    }
   ],
   "source": [
    "# Load from local data/ folder (downloaded by scripts/download_data.py)\n",
    "ds_small = load_from_disk(str(DATA_DIR / \"openwebtext-10k\"))\n",
    "print(f\"Documents: {len(ds_small)}\")\n",
    "print(f\"Columns: {ds_small.column_names}\")\n",
    "print(f\"\\nExample document (first 500 chars):\")\n",
    "print(ds_small[0][\"text\"][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Remove Eval/Test Overlaps Before EDA\n",
    "\n",
    "The instructors require that our exploratory analysis already excludes anything that might leak into the evaluation set.\n",
    "Run the cell below to build the n-gram index from Wikitext-103 (and the optional NLP26 split) and filter the 10k sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(text: str, n: int = 13) -> set[str]:\n",
    "    text = text.lower().strip()\n",
    "    if len(text) < n:\n",
    "        return set()\n",
    "    return {text[i:i+n] for i in range(len(text) - n + 1)}\n",
    "\n",
    "def build_test_ngrams(data_dir: Path) -> set[str]:\n",
    "    ngrams: set[str] = set()\n",
    "    wiki_path = data_dir / 'wikitext-103-test'\n",
    "    if wiki_path.exists():\n",
    "        wiki_test = load_from_disk(str(wiki_path))\n",
    "        for doc in wiki_test:\n",
    "            if doc['text'].strip():\n",
    "                ngrams.update(extract_ngrams(doc['text']))\n",
    "    else:\n",
    "        print('[WARN] Missing wikitext-103-test; run src/scripts/download_data.py')\n",
    "    owt_eval_path = data_dir / 'owt-eval' / 'NLP26' / 'NLP26_OWT_eval' / 'test'\n",
    "    if owt_eval_path.exists():\n",
    "        owt_eval = load_from_disk(str(owt_eval_path))\n",
    "        for doc in owt_eval:\n",
    "            if doc['text'].strip():\n",
    "                ngrams.update(extract_ngrams(doc['text']))\n",
    "    else:\n",
    "        print('[INFO] NLP26 eval split not found locally (optional download).')\n",
    "    print(f'Test-set n-grams: {len(ngrams):,}')\n",
    "    return ngrams\n",
    "\n",
    "def is_contaminated(text: str, test_ngrams: set[str], n: int = 13, threshold: float = 0.8) -> bool:\n",
    "    if not test_ngrams:\n",
    "        return False\n",
    "    doc_ngrams = extract_ngrams(text, n)\n",
    "    if not doc_ngrams:\n",
    "        return False\n",
    "    overlap = len(doc_ngrams & test_ngrams) / len(doc_ngrams)\n",
    "    return overlap > threshold\n",
    "\n",
    "test_ngrams = build_test_ngrams(DATA_DIR)\n",
    "\n",
    "if test_ngrams:\n",
    "    original_len = len(ds_small)\n",
    "    ds_small = ds_small.filter(lambda sample: not is_contaminated(sample['text'], test_ngrams))\n",
    "    print(f'Removed {original_len - len(ds_small)} contaminated docs ({(original_len - len(ds_small)) / original_len * 100:.2f}%).')\n",
    "else:\n",
    "    print('Skipping filtering because no test-set data was available.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenizer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 50257\n",
      "Example: 'Hello world' -> [15496, 995]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"Example: 'Hello world' -> {tokenizer.encode('Hello world')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. EDA: Document & Token Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1217 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in 10k subset: 11,133,993\n",
      "Mean doc length: 1113 tokens\n",
      "Median doc length: 716 tokens\n",
      "Min: 148, Max: 45207\n",
      "Docs > 1024 tokens: 3270 (32.7%)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize a sample and look at length distribution\n",
    "doc_lengths = []\n",
    "for doc in ds_small:\n",
    "    tokens = tokenizer.encode(doc[\"text\"])\n",
    "    doc_lengths.append(len(tokens))\n",
    "\n",
    "doc_lengths = np.array(doc_lengths)\n",
    "print(f\"Total tokens in 10k subset: {doc_lengths.sum():,}\")\n",
    "print(f\"Mean doc length: {doc_lengths.mean():.0f} tokens\")\n",
    "print(f\"Median doc length: {np.median(doc_lengths):.0f} tokens\")\n",
    "print(f\"Min: {doc_lengths.min()}, Max: {doc_lengths.max()}\")\n",
    "print(f\"Docs > 1024 tokens: {(doc_lengths > 1024).sum()} ({(doc_lengths > 1024).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated total tokens in full OWT: 8,922,524,794 (~8.9B)\n"
     ]
    }
   ],
   "source": [
    "# Extrapolate to full dataset\n",
    "# Full OWT has ~8M docs. Our 10k sample gives us a rough estimate.\n",
    "full_docs = 8_013_769\n",
    "est_total_tokens = int(doc_lengths.mean() * full_docs)\n",
    "print(f\"Estimated total tokens in full OWT: {est_total_tokens:,} (~{est_total_tokens/1e9:.1f}B)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Language Detection\n",
    "\n",
    "Filter out non-English documents. Use fasttext's language detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fasttext'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfasttext\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Load from local data/ folder (downloaded by scripts/download_data.py)\u001b[39;00m\n\u001b[32m      4\u001b[39m model_path = \u001b[38;5;28mstr\u001b[39m(DATA_DIR / \u001b[33m\"\u001b[39m\u001b[33mlid.176.ftz\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'fasttext'"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "\n",
    "# Load from local data/ folder (downloaded by scripts/download_data.py)\n",
    "model_path = str(DATA_DIR / \"lid.176.ftz\")\n",
    "lang_model = fasttext.load_model(model_path)\n",
    "print(f\"Loaded language detection model from {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text, model):\n",
    "    \"\"\"Return detected language code and confidence.\"\"\"\n",
    "    # fasttext needs single line, no newlines\n",
    "    first_line = text.split(\"\\n\")[0].strip()\n",
    "    if not first_line:\n",
    "        return \"unknown\", 0.0\n",
    "    pred = model.predict(first_line)\n",
    "    lang = pred[0][0].replace(\"__label__\", \"\")\n",
    "    conf = pred[1][0]\n",
    "    return lang, conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lang_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m non_english = []\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(ds_small):\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     lang, conf = detect_language(doc[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m], \u001b[43mlang_model\u001b[49m)\n\u001b[32m      7\u001b[39m     lang_counts[lang] += \u001b[32m1\u001b[39m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m lang != \u001b[33m\"\u001b[39m\u001b[33men\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mNameError\u001b[39m: name 'lang_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Check language distribution in our sample\n",
    "lang_counts = Counter()\n",
    "non_english = []\n",
    "\n",
    "for i, doc in enumerate(ds_small):\n",
    "    lang, conf = detect_language(doc[\"text\"], lang_model)\n",
    "    lang_counts[lang] += 1\n",
    "    if lang != \"en\":\n",
    "        non_english.append((i, lang, conf, doc[\"text\"][:100]))\n",
    "\n",
    "print(\"Language distribution (top 10):\")\n",
    "for lang, count in lang_counts.most_common(10):\n",
    "    print(f\"  {lang}: {count} ({count/len(ds_small)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nNon-English documents: {len(non_english)} ({len(non_english)/len(ds_small)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample non-English documents:\n"
     ]
    }
   ],
   "source": [
    "# Look at some non-English examples\n",
    "print(\"Sample non-English documents:\")\n",
    "for idx, lang, conf, preview in non_english[:5]:\n",
    "    print(f\"  [{lang} conf={conf:.2f}] {preview}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Formatting & Noise Inspection\n",
    "\n",
    "We need evidence-backed cleaning rules before touching the Python preprocessing stage.\n",
    "The cells below scan a sample of OpenWebText for HTML fragments, fenced code blocks,\n",
    "aggregator headers (e.g. `Title:`, `Category:`), stack traces, and corrupted characters.\n",
    "Run this after `src/scripts/download_data.py` has populated `data/openwebtext-10k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML_TAG_RE = re.compile(r'<\\/?[a-zA-Z][^>]{0,200}>')\n",
    "CODE_BLOCK_RE = re.compile(r'```[\\s\\S]+?```|<code>|</code>|&lt;code&gt;')\n",
    "HEADER_RE = re.compile(r'^(?:title|category|comments?|permalink|posted by|tags):', re.IGNORECASE | re.MULTILINE)\n",
    "STACKTRACE_RE = re.compile(r'Traceback \\(most recent call last\\)|Exception in thread', re.IGNORECASE)\n",
    "WEIRD_CHAR_RE = re.compile(r'[^^\\t\\n\\r\\x20-\\x7E]')\n",
    "\n",
    "def detect_artifacts(text: str) -> list[str]:\n",
    "    \"\"\"Return artifact tags present in the text.\"\"\"\n",
    "    tags = []\n",
    "    if HTML_TAG_RE.search(text):\n",
    "        tags.append('html')\n",
    "    if CODE_BLOCK_RE.search(text):\n",
    "        tags.append('code')\n",
    "    if HEADER_RE.search(text):\n",
    "        tags.append('header')\n",
    "    if STACKTRACE_RE.search(text):\n",
    "        tags.append('stacktrace')\n",
    "    if WEIRD_CHAR_RE.search(text):\n",
    "        tags.append('non_ascii')\n",
    "    return tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DOCS = 2000  # keep lightweight\n",
    "artifact_counts = Counter()\n",
    "artifact_examples = defaultdict(list)\n",
    "\n",
    "for idx, doc in enumerate(ds_small):\n",
    "    if idx >= MAX_DOCS:\n",
    "        break\n",
    "    tags = detect_artifacts(doc['text'])\n",
    "    for tag in tags:\n",
    "        artifact_counts[tag] += 1\n",
    "        if len(artifact_examples[tag]) < 5:\n",
    "            preview = doc['text'][:280].replace('\n",
    "', ' ')\n",
    "            artifact_examples[tag].append((idx, preview))\n",
    "\n",
    "print('Artifact counts (sample of', min(MAX_DOCS, len(ds_small)), 'docs):')\n",
    "for tag, count in artifact_counts.most_common():\n",
    "    pct = count / min(MAX_DOCS, len(ds_small)) * 100\n",
    "    print(f'  {tag}: {count} ({pct:.1f}%)')\n",
    "\n",
    "if not artifact_counts:\n",
    "    print('No artifacts detected in the inspected sample.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag, rows in artifact_examples.items():\n",
    "    print(f'\n",
    "Examples for {tag.upper()}:')\n",
    "    for idx, preview in rows:\n",
    "        print(f'  [doc #{idx}] {preview[:250]}...')\n",
    "    if not rows:\n",
    "        print('  (no samples captured)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa269d1e",
   "metadata": {},
   "source": [
    "### Boilerplate & Metadata Markers\n",
    "Scraped dumps often contain Gutenberg disclaimers or RSS metadata headers that add no value for LAMBADA/HellaSwag/OpenBookQA. The cell below quantifies how frequently these markers appear in our sample and captures examples so we know what to strip during preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34ce446",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOILERPLATE_EXACT_MARKERS = (\n",
    "    '<<< begin of text >>>',\n",
    "    '<<< end of text >>>',\n",
    "    '*** start of this project gutenberg ebook',\n",
    "    '*** end of this project gutenberg ebook',\n",
    "    '*** start of the project gutenberg ebook',\n",
    "    '*** end of the project gutenberg ebook',\n",
    ")\n",
    "BOILERPLATE_PREFIX_MARKERS = (\n",
    "    '*** start of',\n",
    "    '*** end of',\n",
    "    'article url:',\n",
    "    'source:',\n",
    "    'category:',\n",
    "    'tags:',\n",
    "    'title:',\n",
    "    'url:',\n",
    ")\n",
    "\n",
    "def find_boilerplate_markers(text: str) -> list[tuple[str, str]]:\n",
    "    matches = []\n",
    "    for raw_line in text.splitlines():\n",
    "        line = raw_line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        lower = line.lower()\n",
    "        if lower in BOILERPLATE_EXACT_MARKERS:\n",
    "            matches.append(('boilerplate_exact', line))\n",
    "            continue\n",
    "        for prefix in BOILERPLATE_PREFIX_MARKERS:\n",
    "            if lower.startswith(prefix):\n",
    "                tag = f\"boilerplate_prefix:{prefix.rstrip(':')}\"\n",
    "                matches.append((tag, line))\n",
    "                break\n",
    "    return matches\n",
    "\n",
    "boilerplate_counts = Counter()\n",
    "boilerplate_samples = defaultdict(list)\n",
    "for idx, doc in enumerate(ds_small):\n",
    "    if idx >= MAX_DOCS:\n",
    "        break\n",
    "    for tag, line in find_boilerplate_markers(doc['text']):\n",
    "        boilerplate_counts[tag] += 1\n",
    "        if len(boilerplate_samples[tag]) < 3:\n",
    "            boilerplate_samples[tag].append((idx, line[:240]))\n",
    "\n",
    "if not boilerplate_counts:\n",
    "    print('No boilerplate markers detected in the inspected sample.')\n",
    "else:\n",
    "    print('Boilerplate markers (first', min(MAX_DOCS, len(ds_small)), 'docs):')\n",
    "    for tag, count in boilerplate_counts.most_common():\n",
    "        pct = count / min(MAX_DOCS, len(ds_small)) * 100\n",
    "        print(f\"  {tag}: {count} ({pct:.1f}%)\")\n",
    "    for tag, samples in boilerplate_samples.items():\n",
    "        print(f\"\n",
    "Examples for {tag}:\")\n",
    "        for idx, line in samples:\n",
    "            print(f\"  [doc #{idx}] {line}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a972871c",
   "metadata": {},
   "source": [
    "### Duplicate Fingerprints\n",
    "Duplicated or near-duplicated articles waste model capacity and can inflate benchmark scores. We fingerprint normalized text with SHA-1 to see how many docs in the sampled subset would be dropped by our deduplication pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c227737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import unicodedata\n",
    "\n",
    "FINGERPRINT_WHITESPACE_RE = re.compile(r'\\s+')\n",
    "\n",
    "def normalized_fingerprint(text: str) -> str | None:\n",
    "    normalized = unicodedata.normalize('NFC', text or '')\n",
    "    normalized = normalized.lower()\n",
    "    normalized = FINGERPRINT_WHITESPACE_RE.sub(' ', normalized)\n",
    "    normalized = normalized.strip()\n",
    "    if not normalized:\n",
    "        return None\n",
    "    return hashlib.sha1(normalized.encode('utf-8')).hexdigest()\n",
    "\n",
    "fingerprint_to_docs = defaultdict(list)\n",
    "DUP_SCAN_LIMIT = 5000\n",
    "for idx, doc in enumerate(ds_small):\n",
    "    if idx >= DUP_SCAN_LIMIT:\n",
    "        break\n",
    "    fingerprint = normalized_fingerprint(doc['text'])\n",
    "    if fingerprint is None:\n",
    "        continue\n",
    "    fingerprint_to_docs[fingerprint].append(idx)\n",
    "\n",
    "duplicate_clusters = {fp: ids for fp, ids in fingerprint_to_docs.items() if len(ids) > 1}\n",
    "if not duplicate_clusters:\n",
    "    print('No duplicates detected in the first', DUP_SCAN_LIMIT, 'documents.')\n",
    "else:\n",
    "    dup_docs = sum(len(ids) for ids in duplicate_clusters.values())\n",
    "    print(f\"Duplicate fingerprints: {len(duplicate_clusters)} covering {dup_docs} docs\")\n",
    "    preview_items = list(duplicate_clusters.items())[:5]\n",
    "    for fp, ids in preview_items:\n",
    "        preview = ds_small[ids[0]]['text'][:200].replace('\n",
    "', ' ')\n",
    "        print(f\"\n",
    "Fingerprint {fp[:12]}... (n={len(ids)} docs, indices={ids[:5]})\")\n",
    "        print(f\"  preview: {preview}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Set Decontamination\n",
    "\n",
    "**CRITICAL:** Remove any overlap between training data and test sets.\n",
    "\n",
    "Test sets to exclude:\n",
    "- Wikitext-103 test split\n",
    "- NLP26 OpenWebText eval split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Wikitext-103 test set from local data/ folder\n",
    "wiki_test = load_from_disk(str(DATA_DIR / \"wikitext-103-test\"))\n",
    "print(f\"Wikitext-103 test documents: {len(wiki_test)}\")\n",
    "print(f\"Example: {wiki_test[0]['text'][:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a set of n-grams from test sets for overlap detection\n",
    "def extract_ngrams(text, n=13):\n",
    "    \"\"\"Extract character-level n-grams from text for contamination detection.\"\"\"\n",
    "    text = text.lower().strip()\n",
    "    return {text[i:i+n] for i in range(len(text) - n + 1)}\n",
    "\n",
    "# Build test set n-gram index\n",
    "test_ngrams = set()\n",
    "for doc in wiki_test:\n",
    "    if doc[\"text\"].strip():\n",
    "        test_ngrams.update(extract_ngrams(doc[\"text\"]))\n",
    "\n",
    "print(f\"Test set n-grams: {len(test_ngrams):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Also load and index the NLP26 OWT eval split\n",
    "# Download from: https://drive.switch.ch/index.php/s/6TLGQFEIkAPJ72K\n",
    "# Place in data/ directory\n",
    "\n",
    "# owt_eval = load_dataset(\"path/to/eval/split\")\n",
    "# for doc in owt_eval:\n",
    "#     test_ngrams.update(extract_ngrams(doc[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_contaminated(text, test_ngrams, n=13, threshold=0.8):\n",
    "    \"\"\"Check if a document has high overlap with test set n-grams.\"\"\"\n",
    "    doc_ngrams = extract_ngrams(text, n)\n",
    "    if not doc_ngrams:\n",
    "        return False\n",
    "    overlap = len(doc_ngrams & test_ngrams) / len(doc_ngrams)\n",
    "    return overlap > threshold\n",
    "\n",
    "# Test on our sample\n",
    "contaminated = sum(1 for doc in ds_small if is_contaminated(doc[\"text\"], test_ngrams))\n",
    "print(f\"Contaminated documents in 10k sample: {contaminated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Full Preprocessing Pipeline (preview)\n",
    "\n",
    "Combine all filters and tokenize. This is what will become `src/data/preprocess.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_document(doc, tokenizer, lang_model, test_ngrams):\n",
    "    \"\"\"Process one document. Returns token IDs or None if filtered out.\"\"\"\n",
    "    text = doc[\"text\"]\n",
    "\n",
    "    # Filter: skip empty\n",
    "    if not text.strip():\n",
    "        return None, \"empty\"\n",
    "\n",
    "    # Filter: English only\n",
    "    lang, conf = detect_language(text, lang_model)\n",
    "    if lang != \"en\" or conf < 0.5:\n",
    "        return None, \"non_english\"\n",
    "\n",
    "    # Filter: test set contamination\n",
    "    if is_contaminated(text, test_ngrams):\n",
    "        return None, \"contaminated\"\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = tokenizer.encode(text)\n",
    "\n",
    "    # Filter: too short\n",
    "    if len(tokens) < 64:\n",
    "        return None, \"too_short\"\n",
    "\n",
    "    return tokens, \"kept\"\n",
    "\n",
    "\n",
    "# Run on sample\n",
    "from collections import Counter\n",
    "stats = Counter()\n",
    "all_tokens = []\n",
    "\n",
    "for doc in ds_small:\n",
    "    tokens, status = preprocess_document(doc, tokenizer, lang_model, test_ngrams)\n",
    "    stats[status] += 1\n",
    "    if tokens is not None:\n",
    "        all_tokens.extend(tokens)\n",
    "\n",
    "print(\"Preprocessing stats:\")\n",
    "for status, count in stats.most_common():\n",
    "    print(f\"  {status}: {count} ({count/len(ds_small)*100:.1f}%)\")\n",
    "print(f\"\\nTotal tokens kept: {len(all_tokens):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save as Binary\n",
    "\n",
    "Save tokenized data as a memory-mapped numpy array for fast training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy uint16 (vocab size 50257 fits in uint16 max=65535)\n",
    "token_array = np.array(all_tokens, dtype=np.uint16)\n",
    "print(f\"Array shape: {token_array.shape}\")\n",
    "print(f\"Size on disk: {token_array.nbytes / 1e6:.1f} MB\")\n",
    "\n",
    "# Save (for the real pipeline, split into train.bin and val.bin)\n",
    "# out_dir = DATA_DIR / \"processed\"\n",
    "# out_dir.mkdir(exist_ok=True)\n",
    "# token_array.tofile(str(out_dir / \"train.bin\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Run this on the **full** OpenWebText dataset: `load_dataset(\"Skylion007/openwebtext\")`\n",
    "2. Add the NLP26 eval split to decontamination\n",
    "3. Create train/val split (99%/1%)\n",
    "4. Extract working code into `src/data/preprocess.py`\n",
    "5. Wire into `python main.py --stage preprocess --dataset-size full --lang en`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
